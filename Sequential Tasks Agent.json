{"id":"37e6036f-9e04-4e26-b83c-5783a46b98c5","data":{"nodes":[{"data":{"description":"Represents a group of agents, defining how they should collaborate and the tasks they should perform.","display_name":"Sequential Crew","id":"SequentialCrewComponent-AoXgm","node":{"base_classes":["Message"],"beta":false,"conditional_paths":[],"custom_fields":{},"description":"Represents a group of agents with tasks that are executed sequentially.","display_name":"Sequential Crew","documentation":"https://docs.crewai.com/how-to/LLM-Connections/","edited":false,"field_order":["verbose","memory","use_cache","max_rpm","share_crew","function_calling_llm","tasks"],"frozen":false,"icon":"CrewAI","lf_version":"1.0.17","output_types":[],"outputs":[{"cache":true,"display_name":"Output","method":"build_output","name":"output","selected":"Message","types":["Message"],"value":"__UNDEFINED__"}],"pinned":false,"template":{"_type":"Component","code":{"advanced":true,"dynamic":true,"fileTypes":[],"file_path":"","info":"","list":false,"load_from_db":false,"multiline":true,"name":"code","password":false,"placeholder":"","required":true,"show":true,"title_case":false,"type":"code","value":"from crewai import Agent, Crew, Process, Task  # type: ignore\n\nfrom langflow.base.agents.crewai.crew import BaseCrewComponent\nfrom langflow.io import HandleInput\nfrom langflow.schema.message import Message\n\n\nclass SequentialCrewComponent(BaseCrewComponent):\n    display_name: str = \"Sequential Crew\"\n    description: str = \"Represents a group of agents with tasks that are executed sequentially.\"\n    documentation: str = \"https://docs.crewai.com/how-to/Sequential/\"\n    icon = \"CrewAI\"\n\n    inputs = BaseCrewComponent._base_inputs + [\n        HandleInput(name=\"tasks\", display_name=\"Tasks\", input_types=[\"SequentialTask\"], is_list=True),\n    ]\n\n    def get_tasks_and_agents(self) -> tuple[list[Task], list[Agent]]:\n        return self.tasks, [task.agent for task in self.tasks]\n\n    def build_crew(self) -> Message:\n        tasks, agents = self.get_tasks_and_agents()\n        crew = Crew(\n            agents=agents,\n            tasks=tasks,\n            process=Process.sequential,\n            verbose=self.verbose,\n            memory=self.memory,\n            cache=self.use_cache,\n            max_rpm=self.max_rpm,\n            share_crew=self.share_crew,\n            function_calling_llm=self.function_calling_llm,\n            step_callback=self.get_step_callback(),\n            task_callback=self.get_task_callback(),\n        )\n        return crew\n"},"function_calling_llm":{"advanced":true,"display_name":"Function Calling LLM","dynamic":false,"info":"Turns the ReAct CrewAI agent into a function-calling agent","input_types":["LanguageModel"],"list":false,"name":"function_calling_llm","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_metadata":true,"type":"other","value":""},"max_rpm":{"advanced":true,"display_name":"Max RPM","dynamic":false,"info":"","list":false,"name":"max_rpm","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_metadata":true,"type":"int","value":100},"memory":{"advanced":true,"display_name":"Memory","dynamic":false,"info":"","list":false,"name":"memory","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_metadata":true,"type":"bool","value":false},"share_crew":{"advanced":true,"display_name":"Share Crew","dynamic":false,"info":"","list":false,"name":"share_crew","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_metadata":true,"type":"bool","value":false},"tasks":{"advanced":false,"display_name":"Tasks","dynamic":false,"info":"","input_types":["SequentialTask"],"list":true,"name":"tasks","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_metadata":true,"type":"other","value":""},"use_cache":{"advanced":true,"display_name":"Cache","dynamic":false,"info":"","list":false,"name":"use_cache","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_metadata":true,"type":"bool","value":true},"verbose":{"advanced":true,"display_name":"Verbose","dynamic":false,"info":"","list":false,"name":"verbose","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_metadata":true,"type":"int","value":0}}},"type":"SequentialCrewComponent"},"dragging":false,"height":285,"id":"SequentialCrewComponent-AoXgm","position":{"x":1452.9740869513873,"y":216.6316474780062},"positionAbsolute":{"x":1452.9740869513873,"y":216.6316474780062},"selected":false,"type":"genericNode","width":384},{"data":{"description":"Display a chat message in the Playground.","display_name":"Chat Output","id":"ChatOutput-kcdMZ","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.base.io.chat import ChatComponent\r\nfrom langflow.inputs import BoolInput\r\nfrom langflow.io import DropdownInput, MessageTextInput, Output\r\nfrom langflow.memory import store_message\r\nfrom langflow.schema.message import Message\r\nfrom langflow.utils.constants import MESSAGE_SENDER_NAME_AI, MESSAGE_SENDER_USER, MESSAGE_SENDER_AI\r\n\r\n\r\nclass ChatOutput(ChatComponent):\r\n    display_name = \"Chat Output\"\r\n    description = \"Display a chat message in the Playground.\"\r\n    icon = \"ChatOutput\"\r\n    name = \"ChatOutput\"\r\n\r\n    inputs = [\r\n        MessageTextInput(\r\n            name=\"input_value\",\r\n            display_name=\"Text\",\r\n            info=\"Message to be passed as output.\",\r\n        ),\r\n        BoolInput(\r\n            name=\"should_store_message\",\r\n            display_name=\"Store Messages\",\r\n            info=\"Store the message in the history.\",\r\n            value=True,\r\n            advanced=True,\r\n        ),\r\n        DropdownInput(\r\n            name=\"sender\",\r\n            display_name=\"Sender Type\",\r\n            options=[MESSAGE_SENDER_AI, MESSAGE_SENDER_USER],\r\n            value=MESSAGE_SENDER_AI,\r\n            advanced=True,\r\n            info=\"Type of sender.\",\r\n        ),\r\n        MessageTextInput(\r\n            name=\"sender_name\",\r\n            display_name=\"Sender Name\",\r\n            info=\"Name of the sender.\",\r\n            value=MESSAGE_SENDER_NAME_AI,\r\n            advanced=True,\r\n        ),\r\n        MessageTextInput(\r\n            name=\"session_id\",\r\n            display_name=\"Session ID\",\r\n            info=\"The session ID of the chat. If empty, the current session ID parameter will be used.\",\r\n            advanced=True,\r\n        ),\r\n        MessageTextInput(\r\n            name=\"data_template\",\r\n            display_name=\"Data Template\",\r\n            value=\"{text}\",\r\n            advanced=True,\r\n            info=\"Template to convert Data to Text. If left empty, it will be dynamically set to the Data's text key.\",\r\n        ),\r\n    ]\r\n    outputs = [\r\n        Output(display_name=\"Message\", name=\"message\", method=\"message_response\"),\r\n    ]\r\n\r\n    def message_response(self) -> Message:\r\n        message = Message(\r\n            text=self.input_value,\r\n            sender=self.sender,\r\n            sender_name=self.sender_name,\r\n            session_id=self.session_id,\r\n        )\r\n        if (\r\n            self.session_id\r\n            and isinstance(message, Message)\r\n            and isinstance(message.text, str)\r\n            and self.should_store_message\r\n        ):\r\n            store_message(\r\n                message,\r\n                flow_id=self.graph.flow_id,\r\n            )\r\n            self.message.value = message\r\n\r\n        self.status = message\r\n        return message\r\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"data_template":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"data_template","value":"{text}","display_name":"Data Template","advanced":true,"input_types":["Message"],"dynamic":false,"info":"Template to convert Data to Text. If left empty, it will be dynamically set to the Data's text key.","title_case":false,"type":"str","_input_type":"MessageTextInput"},"input_value":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"input_value","value":"","display_name":"Text","advanced":false,"input_types":["Message"],"dynamic":false,"info":"Message to be passed as output.","title_case":false,"type":"str","_input_type":"MessageTextInput"},"sender":{"trace_as_metadata":true,"options":["Machine","User"],"combobox":false,"required":false,"placeholder":"","show":true,"name":"sender","value":"Machine","display_name":"Sender Type","advanced":true,"dynamic":false,"info":"Type of sender.","title_case":false,"type":"str","_input_type":"DropdownInput"},"sender_name":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"sender_name","value":"AI","display_name":"Sender Name","advanced":true,"input_types":["Message"],"dynamic":false,"info":"Name of the sender.","title_case":false,"type":"str","_input_type":"MessageTextInput"},"session_id":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"session_id","value":"","display_name":"Session ID","advanced":true,"input_types":["Message"],"dynamic":false,"info":"The session ID of the chat. If empty, the current session ID parameter will be used.","title_case":false,"type":"str","_input_type":"MessageTextInput"},"should_store_message":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"should_store_message","value":true,"display_name":"Store Messages","advanced":true,"dynamic":false,"info":"Store the message in the history.","title_case":false,"type":"bool","_input_type":"BoolInput"}},"description":"Display a chat message in the Playground.","icon":"ChatOutput","base_classes":["Message"],"display_name":"Chat Output","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"message","display_name":"Message","method":"message_response","value":"__UNDEFINED__","cache":true}],"field_order":["input_value","should_store_message","sender","sender_name","session_id","data_template"],"beta":false,"edited":true},"type":"ChatOutput"},"dragging":false,"height":299,"id":"ChatOutput-kcdMZ","position":{"x":1937.272512235776,"y":227.4117341237682},"selected":false,"type":"genericNode","width":384,"positionAbsolute":{"x":1937.272512235776,"y":227.4117341237682}},{"data":{"id":"TextInput-q5KbN","node":{"base_classes":["Message"],"beta":false,"conditional_paths":[],"custom_fields":{},"description":"Get text inputs from the Playground.","display_name":"Topic","documentation":"","edited":false,"field_order":["input_value"],"frozen":false,"icon":"type","lf_version":"1.0.17","output_types":[],"outputs":[{"cache":true,"display_name":"Text","method":"text_response","name":"text","selected":"Message","types":["Message"],"value":"__UNDEFINED__"}],"pinned":false,"template":{"_type":"Component","code":{"advanced":true,"dynamic":true,"fileTypes":[],"file_path":"","info":"","list":false,"load_from_db":false,"multiline":true,"name":"code","password":false,"placeholder":"","required":true,"show":true,"title_case":false,"type":"code","value":"from langflow.base.io.text import TextComponent\nfrom langflow.io import MessageTextInput, Output\nfrom langflow.schema.message import Message\n\n\nclass TextInputComponent(TextComponent):\n    display_name = \"Text Input\"\n    description = \"Get text inputs from the Playground.\"\n    icon = \"type\"\n    name = \"TextInput\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"input_value\",\n            display_name=\"Text\",\n            info=\"Text to be passed as input.\",\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Text\", name=\"text\", method=\"text_response\"),\n    ]\n\n    def text_response(self) -> Message:\n        message = Message(\n            text=self.input_value,\n        )\n        return message\n"},"input_value":{"advanced":false,"display_name":"Text","dynamic":false,"info":"Text to be passed as input.","input_types":["Message"],"list":false,"load_from_db":false,"name":"input_value","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_input":true,"trace_as_metadata":true,"type":"str","value":""}}},"type":"TextInput"},"dragging":false,"height":299,"id":"TextInput-q5KbN","position":{"x":-2044.1039646921665,"y":291.77565151149054},"positionAbsolute":{"x":-2044.1039646921665,"y":291.77565151149054},"selected":false,"type":"genericNode","width":384},{"data":{"description":"Create a prompt template with dynamic variables.","display_name":"Prompt","id":"Prompt-m0j1W","node":{"base_classes":["Message"],"beta":false,"conditional_paths":[],"custom_fields":{"template":["topic"]},"description":"Create a prompt template with dynamic variables.","display_name":"Prompt","documentation":"","edited":false,"field_order":["template"],"frozen":false,"icon":"prompts","lf_version":"1.0.17","output_types":[],"outputs":[{"cache":true,"display_name":"Prompt Message","method":"build_prompt","name":"prompt","selected":"Message","types":["Message"],"value":"__UNDEFINED__"}],"pinned":false,"template":{"_type":"Component","code":{"advanced":true,"dynamic":true,"fileTypes":[],"file_path":"","info":"","list":false,"load_from_db":false,"multiline":true,"name":"code","password":false,"placeholder":"","required":true,"show":true,"title_case":false,"type":"code","value":"from langflow.base.prompts.api_utils import process_prompt_template\nfrom langflow.custom import Component\nfrom langflow.inputs.inputs import DefaultPromptField\nfrom langflow.io import Output, PromptInput\nfrom langflow.schema.message import Message\nfrom langflow.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"prompts\"\n    trace_type = \"prompt\"\n    name = \"Prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt Message\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(\n        self,\n    ) -> Message:\n        prompt = await Message.from_template_and_variables(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def _update_template(self, frontend_node: dict):\n        prompt_template = frontend_node[\"template\"][\"template\"][\"value\"]\n        custom_fields = frontend_node[\"custom_fields\"]\n        frontend_node_template = frontend_node[\"template\"]\n        _ = process_prompt_template(\n            template=prompt_template,\n            name=\"template\",\n            custom_fields=custom_fields,\n            frontend_node_template=frontend_node_template,\n        )\n        return frontend_node\n\n    def post_code_processing(self, new_frontend_node: dict, current_frontend_node: dict):\n        \"\"\"\n        This function is called after the code validation is done.\n        \"\"\"\n        frontend_node = super().post_code_processing(new_frontend_node, current_frontend_node)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        # Kept it duplicated for backwards compatibility\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_frontend_node\n        # and update the frontend_node with those values\n        update_template_values(new_template=frontend_node, previous_template=current_frontend_node[\"template\"])\n        return frontend_node\n\n    def _get_fallback_input(self, **kwargs):\n        return DefaultPromptField(**kwargs)\n"},"template":{"advanced":false,"display_name":"Template","dynamic":false,"info":"","list":false,"load_from_db":false,"name":"template","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_input":true,"type":"prompt","value":"Topic: {topic}\n\nBuild a document about this document."},"topic":{"advanced":false,"display_name":"topic","dynamic":false,"field_type":"str","fileTypes":[],"file_path":"","info":"","input_types":["Message","Text"],"list":false,"load_from_db":false,"multiline":true,"name":"topic","password":false,"placeholder":"","required":false,"show":true,"title_case":false,"type":"str","value":""}}},"type":"Prompt"},"dragging":false,"height":414,"id":"Prompt-m0j1W","position":{"x":-1154.4124217561132,"y":813.2475923059123},"positionAbsolute":{"x":-1154.4124217561132,"y":813.2475923059123},"selected":false,"type":"genericNode","width":384},{"data":{"description":"Create a prompt template with dynamic variables.","display_name":"Prompt","id":"Prompt-vu8ox","node":{"base_classes":["Message"],"beta":false,"conditional_paths":[],"custom_fields":{"template":["topic"]},"description":"Create a prompt template with dynamic variables.","display_name":"Prompt","documentation":"","edited":false,"field_order":["template"],"frozen":false,"icon":"prompts","lf_version":"1.0.17","output_types":[],"outputs":[{"cache":true,"display_name":"Prompt Message","method":"build_prompt","name":"prompt","selected":"Message","types":["Message"],"value":"__UNDEFINED__"}],"pinned":false,"template":{"_type":"Component","code":{"advanced":true,"dynamic":true,"fileTypes":[],"file_path":"","info":"","list":false,"load_from_db":false,"multiline":true,"name":"code","password":false,"placeholder":"","required":true,"show":true,"title_case":false,"type":"code","value":"from langflow.base.prompts.api_utils import process_prompt_template\nfrom langflow.custom import Component\nfrom langflow.inputs.inputs import DefaultPromptField\nfrom langflow.io import Output, PromptInput\nfrom langflow.schema.message import Message\nfrom langflow.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"prompts\"\n    trace_type = \"prompt\"\n    name = \"Prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt Message\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(\n        self,\n    ) -> Message:\n        prompt = await Message.from_template_and_variables(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def _update_template(self, frontend_node: dict):\n        prompt_template = frontend_node[\"template\"][\"template\"][\"value\"]\n        custom_fields = frontend_node[\"custom_fields\"]\n        frontend_node_template = frontend_node[\"template\"]\n        _ = process_prompt_template(\n            template=prompt_template,\n            name=\"template\",\n            custom_fields=custom_fields,\n            frontend_node_template=frontend_node_template,\n        )\n        return frontend_node\n\n    def post_code_processing(self, new_frontend_node: dict, current_frontend_node: dict):\n        \"\"\"\n        This function is called after the code validation is done.\n        \"\"\"\n        frontend_node = super().post_code_processing(new_frontend_node, current_frontend_node)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        # Kept it duplicated for backwards compatibility\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_frontend_node\n        # and update the frontend_node with those values\n        update_template_values(new_template=frontend_node, previous_template=current_frontend_node[\"template\"])\n        return frontend_node\n\n    def _get_fallback_input(self, **kwargs):\n        return DefaultPromptField(**kwargs)\n"},"template":{"advanced":false,"display_name":"Template","dynamic":false,"info":"","list":false,"load_from_db":false,"name":"template","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_input":true,"type":"prompt","value":"Topic: {topic}\n\nRevise this document."},"topic":{"advanced":false,"display_name":"topic","dynamic":false,"field_type":"str","fileTypes":[],"file_path":"","info":"","input_types":["Message","Text"],"list":false,"load_from_db":false,"multiline":true,"name":"topic","password":false,"placeholder":"","required":false,"show":true,"title_case":false,"type":"str","value":""}}},"type":"Prompt"},"dragging":false,"height":414,"id":"Prompt-vu8ox","position":{"x":-369.56336473301,"y":790.2887357303061},"positionAbsolute":{"x":-369.56336473301,"y":790.2887357303061},"selected":false,"type":"genericNode","width":384},{"data":{"description":"Create a prompt template with dynamic variables.","display_name":"Prompt","id":"Prompt-ajPtk","node":{"template":{"_type":"Component","code":{"advanced":true,"dynamic":true,"fileTypes":[],"file_path":"","info":"","list":false,"load_from_db":false,"multiline":true,"name":"code","password":false,"placeholder":"","required":true,"show":true,"title_case":false,"type":"code","value":"from langflow.base.prompts.api_utils import process_prompt_template\nfrom langflow.custom import Component\nfrom langflow.inputs.inputs import DefaultPromptField\nfrom langflow.io import Output, PromptInput\nfrom langflow.schema.message import Message\nfrom langflow.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"prompts\"\n    trace_type = \"prompt\"\n    name = \"Prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt Message\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(\n        self,\n    ) -> Message:\n        prompt = await Message.from_template_and_variables(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def _update_template(self, frontend_node: dict):\n        prompt_template = frontend_node[\"template\"][\"template\"][\"value\"]\n        custom_fields = frontend_node[\"custom_fields\"]\n        frontend_node_template = frontend_node[\"template\"]\n        _ = process_prompt_template(\n            template=prompt_template,\n            name=\"template\",\n            custom_fields=custom_fields,\n            frontend_node_template=frontend_node_template,\n        )\n        return frontend_node\n\n    def post_code_processing(self, new_frontend_node: dict, current_frontend_node: dict):\n        \"\"\"\n        This function is called after the code validation is done.\n        \"\"\"\n        frontend_node = super().post_code_processing(new_frontend_node, current_frontend_node)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        # Kept it duplicated for backwards compatibility\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_frontend_node\n        # and update the frontend_node with those values\n        update_template_values(new_template=frontend_node, previous_template=current_frontend_node[\"template\"])\n        return frontend_node\n\n    def _get_fallback_input(self, **kwargs):\n        return DefaultPromptField(**kwargs)\n"},"template":{"advanced":false,"display_name":"Template","dynamic":false,"info":"","list":false,"load_from_db":false,"name":"template","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_input":true,"type":"prompt","value":"Topic: {topic}\n\nEscreva um resumo conciso, mas informativo, trazendo especialmente as informações mais atuais e dados quantitativos. "},"topic":{"field_type":"str","required":false,"placeholder":"","list":false,"show":true,"multiline":true,"value":"","fileTypes":[],"file_path":"","password":false,"name":"topic","display_name":"topic","advanced":false,"input_types":["Message","Text"],"dynamic":false,"info":"","load_from_db":false,"title_case":false,"type":"str"}},"description":"Create a prompt template with dynamic variables.","icon":"prompts","is_input":null,"is_output":null,"is_composition":null,"base_classes":["Message"],"name":"","display_name":"Prompt","documentation":"","custom_fields":{"template":["topic"]},"output_types":[],"full_path":null,"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"prompt","hidden":null,"display_name":"Prompt Message","method":"build_prompt","value":"__UNDEFINED__","cache":true}],"field_order":["template"],"beta":false,"error":null,"edited":false,"lf_version":"1.0.17"},"type":"Prompt"},"dragging":false,"height":414,"id":"Prompt-ajPtk","position":{"x":383.48176594858205,"y":804.7835051646966},"positionAbsolute":{"x":383.48176594858205,"y":804.7835051646966},"selected":false,"type":"genericNode","width":384},{"data":{"id":"SequentialTaskAgentComponent-pkN2P","node":{"base_classes":["SequentialTask"],"beta":false,"conditional_paths":[],"custom_fields":{},"description":"Creates a CrewAI Task and its associated Agent.","display_name":"Sequential Task Agent","documentation":"https://docs.crewai.com/how-to/LLM-Connections/","edited":false,"field_order":["role","goal","backstory","tools","llm","memory","verbose","allow_delegation","allow_code_execution","agent_kwargs","task_description","expected_output","async_execution","previous_task"],"frozen":false,"icon":"CrewAI","lf_version":"1.0.17","output_types":[],"outputs":[{"cache":true,"display_name":"Sequential Task","method":"build_agent_and_task","name":"task_output","selected":"SequentialTask","types":["SequentialTask"],"value":"__UNDEFINED__"}],"pinned":false,"template":{"_type":"Component","agent_kwargs":{"_input_type":"DictInput","advanced":true,"display_name":"Agent kwargs","dynamic":false,"info":"Additional kwargs for the agent.","list":true,"name":"agent_kwargs","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_input":true,"type":"dict","value":{}},"allow_code_execution":{"_input_type":"BoolInput","advanced":true,"display_name":"Allow Code Execution","dynamic":false,"info":"Whether the agent is allowed to execute code.","list":false,"name":"allow_code_execution","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_metadata":true,"type":"bool","value":false},"allow_delegation":{"_input_type":"BoolInput","advanced":true,"display_name":"Allow Delegation","dynamic":false,"info":"Whether the agent is allowed to delegate tasks to other agents.","list":false,"name":"allow_delegation","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_metadata":true,"type":"bool","value":false},"async_execution":{"_input_type":"BoolInput","advanced":true,"display_name":"Async Execution","dynamic":false,"info":"Boolean flag indicating asynchronous task execution.","list":false,"name":"async_execution","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_metadata":true,"type":"bool","value":false},"backstory":{"_input_type":"MultilineInput","advanced":false,"display_name":"Backstory","dynamic":false,"info":"The backstory of the agent.","input_types":["Message"],"list":false,"load_from_db":false,"multiline":true,"name":"backstory","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_input":true,"trace_as_metadata":true,"type":"str","value":"criar relatórios de informações relevantes, com dados e cenrários economicos."},"code":{"advanced":true,"dynamic":true,"fileTypes":[],"file_path":"","info":"","list":false,"load_from_db":false,"multiline":true,"name":"code","password":false,"placeholder":"","required":true,"show":true,"title_case":false,"type":"code","value":"from crewai import Agent, Task\n\nfrom langflow.base.agents.crewai.tasks import SequentialTask\nfrom langflow.custom import Component\nfrom langflow.io import BoolInput, DictInput, HandleInput, MultilineInput, Output\n\n\nclass SequentialTaskAgentComponent(Component):\n    display_name = \"Sequential Task Agent\"\n    description = \"Creates a CrewAI Task and its associated Agent.\"\n    documentation = \"https://docs.crewai.com/how-to/LLM-Connections/\"\n    icon = \"CrewAI\"\n\n    inputs = [\n        # Agent inputs\n        MultilineInput(name=\"role\", display_name=\"Role\", info=\"The role of the agent.\"),\n        MultilineInput(name=\"goal\", display_name=\"Goal\", info=\"The objective of the agent.\"),\n        MultilineInput(\n            name=\"backstory\",\n            display_name=\"Backstory\",\n            info=\"The backstory of the agent.\",\n        ),\n        HandleInput(\n            name=\"tools\",\n            display_name=\"Tools\",\n            input_types=[\"Tool\"],\n            is_list=True,\n            info=\"Tools at agent's disposal\",\n            value=[],\n        ),\n        HandleInput(\n            name=\"llm\",\n            display_name=\"Language Model\",\n            info=\"Language model that will run the agent.\",\n            input_types=[\"LanguageModel\"],\n        ),\n        BoolInput(\n            name=\"memory\",\n            display_name=\"Memory\",\n            info=\"Whether the agent should have memory or not\",\n            advanced=True,\n            value=True,\n        ),\n        BoolInput(\n            name=\"verbose\",\n            display_name=\"Verbose\",\n            advanced=True,\n            value=True,\n        ),\n        BoolInput(\n            name=\"allow_delegation\",\n            display_name=\"Allow Delegation\",\n            info=\"Whether the agent is allowed to delegate tasks to other agents.\",\n            value=False,\n            advanced=True,\n        ),\n        BoolInput(\n            name=\"allow_code_execution\",\n            display_name=\"Allow Code Execution\",\n            info=\"Whether the agent is allowed to execute code.\",\n            value=False,\n            advanced=True,\n        ),\n        DictInput(\n            name=\"agent_kwargs\",\n            display_name=\"Agent kwargs\",\n            info=\"Additional kwargs for the agent.\",\n            is_list=True,\n            advanced=True,\n        ),\n        # Task inputs\n        MultilineInput(\n            name=\"task_description\",\n            display_name=\"Task Description\",\n            info=\"Descriptive text detailing task's purpose and execution.\",\n        ),\n        MultilineInput(\n            name=\"expected_output\",\n            display_name=\"Expected Task Output\",\n            info=\"Clear definition of expected task outcome.\",\n        ),\n        BoolInput(\n            name=\"async_execution\",\n            display_name=\"Async Execution\",\n            value=False,\n            advanced=True,\n            info=\"Boolean flag indicating asynchronous task execution.\",\n        ),\n        # Chaining input\n        HandleInput(\n            name=\"previous_task\",\n            display_name=\"Previous Task\",\n            input_types=[\"SequentialTask\"],\n            info=\"The previous task in the sequence (for chaining).\",\n            required=False,\n        ),\n    ]\n\n    outputs = [\n        Output(\n            display_name=\"Sequential Task\",\n            name=\"task_output\",\n            method=\"build_agent_and_task\",\n        ),\n    ]\n\n    def build_agent_and_task(self) -> list[SequentialTask]:\n        # Build the agent\n        agent_kwargs = self.agent_kwargs or {}\n        agent = Agent(\n            role=self.role,\n            goal=self.goal,\n            backstory=self.backstory,\n            llm=self.llm,\n            verbose=self.verbose,\n            memory=self.memory,\n            tools=self.tools if self.tools else [],\n            allow_delegation=self.allow_delegation,\n            allow_code_execution=self.allow_code_execution,\n            **agent_kwargs,\n        )\n\n        # Build the task\n        task = Task(\n            description=self.task_description,\n            expected_output=self.expected_output,\n            agent=agent,\n            async_execution=self.async_execution,\n        )\n\n        # If there's a previous task, create a list of tasks\n        if self.previous_task:\n            if isinstance(self.previous_task, list):\n                tasks = self.previous_task + [task]\n            else:\n                tasks = [self.previous_task, task]\n        else:\n            tasks = [task]\n\n        self.status = f\"Agent: {repr(agent)}\\nTask: {repr(task)}\"\n        return tasks\n"},"expected_output":{"_input_type":"MultilineInput","advanced":false,"display_name":"Expected Task Output","dynamic":false,"info":"Clear definition of expected task outcome.","input_types":["Message"],"list":false,"load_from_db":false,"multiline":true,"name":"expected_output","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_input":true,"trace_as_metadata":true,"type":"str","value":"Bullet points and small phrases about the research topic."},"goal":{"_input_type":"MultilineInput","advanced":false,"display_name":"Goal","dynamic":false,"info":"The objective of the agent.","input_types":["Message"],"list":false,"load_from_db":false,"multiline":true,"name":"goal","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_input":true,"trace_as_metadata":true,"type":"str","value":"Buscar noticiais mais atuais, especialmente do mesmo dia "},"llm":{"_input_type":"HandleInput","advanced":false,"display_name":"Language Model","dynamic":false,"info":"Language model that will run the agent.","input_types":["LanguageModel"],"list":false,"name":"llm","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_metadata":true,"type":"other","value":""},"memory":{"_input_type":"BoolInput","advanced":true,"display_name":"Memory","dynamic":false,"info":"Whether the agent should have memory or not","list":false,"name":"memory","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_metadata":true,"type":"bool","value":true},"previous_task":{"_input_type":"HandleInput","advanced":false,"display_name":"Previous Task","dynamic":false,"info":"The previous task in the sequence (for chaining).","input_types":["SequentialTask"],"list":false,"name":"previous_task","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_metadata":true,"type":"other","value":""},"role":{"_input_type":"MultilineInput","advanced":false,"display_name":"Role","dynamic":false,"info":"The role of the agent.","input_types":["Message"],"list":false,"load_from_db":false,"multiline":true,"name":"role","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_input":true,"trace_as_metadata":true,"type":"str","value":"Pesquisador"},"task_description":{"_input_type":"MultilineInput","advanced":false,"display_name":"Task Description","dynamic":false,"info":"Descriptive text detailing task's purpose and execution.","input_types":["Message"],"list":false,"load_from_db":false,"multiline":true,"name":"task_description","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_input":true,"trace_as_metadata":true,"type":"str","value":""},"tools":{"_input_type":"HandleInput","advanced":false,"display_name":"Tools","dynamic":false,"info":"Tools at agent's disposal","input_types":["Tool"],"list":true,"name":"tools","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_metadata":true,"type":"other","value":[]},"verbose":{"_input_type":"BoolInput","advanced":true,"display_name":"Verbose","dynamic":false,"info":"","list":false,"name":"verbose","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_metadata":true,"type":"bool","value":true}}},"type":"SequentialTaskAgentComponent"},"dragging":false,"height":785,"id":"SequentialTaskAgentComponent-pkN2P","position":{"x":-868.8473885441606,"y":-455.69887223696816},"positionAbsolute":{"x":-868.8473885441606,"y":-455.69887223696816},"selected":false,"type":"genericNode","width":384},{"data":{"id":"SequentialTaskAgentComponent-tuwOX","node":{"base_classes":["SequentialTask"],"beta":false,"conditional_paths":[],"custom_fields":{},"description":"Creates a CrewAI Task and its associated Agent.","display_name":"Sequential Task Agent","documentation":"https://docs.crewai.com/how-to/LLM-Connections/","edited":false,"field_order":["role","goal","backstory","tools","llm","memory","verbose","allow_delegation","allow_code_execution","agent_kwargs","task_description","expected_output","async_execution","previous_task"],"frozen":false,"icon":"CrewAI","lf_version":"1.0.17","output_types":[],"outputs":[{"cache":true,"display_name":"Sequential Task","method":"build_agent_and_task","name":"task_output","selected":"SequentialTask","types":["SequentialTask"],"value":"__UNDEFINED__"}],"pinned":false,"template":{"_type":"Component","agent_kwargs":{"_input_type":"DictInput","advanced":true,"display_name":"Agent kwargs","dynamic":false,"info":"Additional kwargs for the agent.","list":true,"name":"agent_kwargs","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_input":true,"type":"dict","value":{}},"allow_code_execution":{"_input_type":"BoolInput","advanced":true,"display_name":"Allow Code Execution","dynamic":false,"info":"Whether the agent is allowed to execute code.","list":false,"name":"allow_code_execution","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_metadata":true,"type":"bool","value":false},"allow_delegation":{"_input_type":"BoolInput","advanced":true,"display_name":"Allow Delegation","dynamic":false,"info":"Whether the agent is allowed to delegate tasks to other agents.","list":false,"name":"allow_delegation","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_metadata":true,"type":"bool","value":false},"async_execution":{"_input_type":"BoolInput","advanced":true,"display_name":"Async Execution","dynamic":false,"info":"Boolean flag indicating asynchronous task execution.","list":false,"name":"async_execution","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_metadata":true,"type":"bool","value":false},"backstory":{"_input_type":"MultilineInput","advanced":false,"display_name":"Backstory","dynamic":false,"info":"The backstory of the agent.","input_types":["Message"],"list":false,"load_from_db":false,"multiline":true,"name":"backstory","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_input":true,"trace_as_metadata":true,"type":"str","value":"You are the editor of the most reputable journal in the world."},"code":{"advanced":true,"dynamic":true,"fileTypes":[],"file_path":"","info":"","list":false,"load_from_db":false,"multiline":true,"name":"code","password":false,"placeholder":"","required":true,"show":true,"title_case":false,"type":"code","value":"from crewai import Agent, Task\n\nfrom langflow.base.agents.crewai.tasks import SequentialTask\nfrom langflow.custom import Component\nfrom langflow.io import BoolInput, DictInput, HandleInput, MultilineInput, Output\n\n\nclass SequentialTaskAgentComponent(Component):\n    display_name = \"Sequential Task Agent\"\n    description = \"Creates a CrewAI Task and its associated Agent.\"\n    documentation = \"https://docs.crewai.com/how-to/LLM-Connections/\"\n    icon = \"CrewAI\"\n\n    inputs = [\n        # Agent inputs\n        MultilineInput(name=\"role\", display_name=\"Role\", info=\"The role of the agent.\"),\n        MultilineInput(name=\"goal\", display_name=\"Goal\", info=\"The objective of the agent.\"),\n        MultilineInput(\n            name=\"backstory\",\n            display_name=\"Backstory\",\n            info=\"The backstory of the agent.\",\n        ),\n        HandleInput(\n            name=\"tools\",\n            display_name=\"Tools\",\n            input_types=[\"Tool\"],\n            is_list=True,\n            info=\"Tools at agent's disposal\",\n            value=[],\n        ),\n        HandleInput(\n            name=\"llm\",\n            display_name=\"Language Model\",\n            info=\"Language model that will run the agent.\",\n            input_types=[\"LanguageModel\"],\n        ),\n        BoolInput(\n            name=\"memory\",\n            display_name=\"Memory\",\n            info=\"Whether the agent should have memory or not\",\n            advanced=True,\n            value=True,\n        ),\n        BoolInput(\n            name=\"verbose\",\n            display_name=\"Verbose\",\n            advanced=True,\n            value=True,\n        ),\n        BoolInput(\n            name=\"allow_delegation\",\n            display_name=\"Allow Delegation\",\n            info=\"Whether the agent is allowed to delegate tasks to other agents.\",\n            value=False,\n            advanced=True,\n        ),\n        BoolInput(\n            name=\"allow_code_execution\",\n            display_name=\"Allow Code Execution\",\n            info=\"Whether the agent is allowed to execute code.\",\n            value=False,\n            advanced=True,\n        ),\n        DictInput(\n            name=\"agent_kwargs\",\n            display_name=\"Agent kwargs\",\n            info=\"Additional kwargs for the agent.\",\n            is_list=True,\n            advanced=True,\n        ),\n        # Task inputs\n        MultilineInput(\n            name=\"task_description\",\n            display_name=\"Task Description\",\n            info=\"Descriptive text detailing task's purpose and execution.\",\n        ),\n        MultilineInput(\n            name=\"expected_output\",\n            display_name=\"Expected Task Output\",\n            info=\"Clear definition of expected task outcome.\",\n        ),\n        BoolInput(\n            name=\"async_execution\",\n            display_name=\"Async Execution\",\n            value=False,\n            advanced=True,\n            info=\"Boolean flag indicating asynchronous task execution.\",\n        ),\n        # Chaining input\n        HandleInput(\n            name=\"previous_task\",\n            display_name=\"Previous Task\",\n            input_types=[\"SequentialTask\"],\n            info=\"The previous task in the sequence (for chaining).\",\n            required=False,\n        ),\n    ]\n\n    outputs = [\n        Output(\n            display_name=\"Sequential Task\",\n            name=\"task_output\",\n            method=\"build_agent_and_task\",\n        ),\n    ]\n\n    def build_agent_and_task(self) -> list[SequentialTask]:\n        # Build the agent\n        agent_kwargs = self.agent_kwargs or {}\n        agent = Agent(\n            role=self.role,\n            goal=self.goal,\n            backstory=self.backstory,\n            llm=self.llm,\n            verbose=self.verbose,\n            memory=self.memory,\n            tools=self.tools if self.tools else [],\n            allow_delegation=self.allow_delegation,\n            allow_code_execution=self.allow_code_execution,\n            **agent_kwargs,\n        )\n\n        # Build the task\n        task = Task(\n            description=self.task_description,\n            expected_output=self.expected_output,\n            agent=agent,\n            async_execution=self.async_execution,\n        )\n\n        # If there's a previous task, create a list of tasks\n        if self.previous_task:\n            if isinstance(self.previous_task, list):\n                tasks = self.previous_task + [task]\n            else:\n                tasks = [self.previous_task, task]\n        else:\n            tasks = [task]\n\n        self.status = f\"Agent: {repr(agent)}\\nTask: {repr(task)}\"\n        return tasks\n"},"expected_output":{"_input_type":"MultilineInput","advanced":false,"display_name":"Expected Task Output","dynamic":false,"info":"Clear definition of expected task outcome.","input_types":["Message"],"list":false,"load_from_db":false,"multiline":true,"name":"expected_output","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_input":true,"trace_as_metadata":true,"type":"str","value":"Small paragraphs and bullet points with the corrected content."},"goal":{"_input_type":"MultilineInput","advanced":false,"display_name":"Goal","dynamic":false,"info":"The objective of the agent.","input_types":["Message"],"list":false,"load_from_db":false,"multiline":true,"name":"goal","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_input":true,"trace_as_metadata":true,"type":"str","value":"You should edit the Information provided by the Researcher to make it more palatable and to not contain misleading information."},"llm":{"_input_type":"HandleInput","advanced":false,"display_name":"Language Model","dynamic":false,"info":"Language model that will run the agent.","input_types":["LanguageModel"],"list":false,"name":"llm","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_metadata":true,"type":"other","value":""},"memory":{"_input_type":"BoolInput","advanced":true,"display_name":"Memory","dynamic":false,"info":"Whether the agent should have memory or not","list":false,"name":"memory","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_metadata":true,"type":"bool","value":true},"previous_task":{"_input_type":"HandleInput","advanced":false,"display_name":"Previous Task","dynamic":false,"info":"The previous task in the sequence (for chaining).","input_types":["SequentialTask"],"list":false,"name":"previous_task","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_metadata":true,"type":"other","value":""},"role":{"_input_type":"MultilineInput","advanced":false,"display_name":"Role","dynamic":false,"info":"The role of the agent.","input_types":["Message"],"list":false,"load_from_db":false,"multiline":true,"name":"role","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_input":true,"trace_as_metadata":true,"type":"str","value":"Editor"},"task_description":{"_input_type":"MultilineInput","advanced":false,"display_name":"Task Description","dynamic":false,"info":"Descriptive text detailing task's purpose and execution.","input_types":["Message"],"list":false,"load_from_db":false,"multiline":true,"name":"task_description","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_input":true,"trace_as_metadata":true,"type":"str","value":""},"tools":{"_input_type":"HandleInput","advanced":false,"display_name":"Tools","dynamic":false,"info":"Tools at agent's disposal","input_types":["Tool"],"list":true,"name":"tools","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_metadata":true,"type":"other","value":[]},"verbose":{"_input_type":"BoolInput","advanced":true,"display_name":"Verbose","dynamic":false,"info":"","list":false,"name":"verbose","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_metadata":true,"type":"bool","value":true}}},"type":"SequentialTaskAgentComponent"},"dragging":false,"height":785,"id":"SequentialTaskAgentComponent-tuwOX","position":{"x":62.10105154443647,"y":-336.82282969954827},"positionAbsolute":{"x":62.10105154443647,"y":-336.82282969954827},"selected":false,"type":"genericNode","width":384},{"data":{"id":"SequentialTaskAgentComponent-saOps","node":{"base_classes":["SequentialTask"],"beta":false,"conditional_paths":[],"custom_fields":{},"description":"Creates a CrewAI Task and its associated Agent.","display_name":"Sequential Task Agent","documentation":"https://docs.crewai.com/how-to/LLM-Connections/","edited":false,"field_order":["role","goal","backstory","tools","llm","memory","verbose","allow_delegation","allow_code_execution","agent_kwargs","task_description","expected_output","async_execution","previous_task"],"frozen":false,"icon":"CrewAI","lf_version":"1.0.17","output_types":[],"outputs":[{"cache":true,"display_name":"Sequential Task","method":"build_agent_and_task","name":"task_output","selected":"SequentialTask","types":["SequentialTask"],"value":"__UNDEFINED__"}],"pinned":false,"template":{"_type":"Component","agent_kwargs":{"_input_type":"DictInput","advanced":true,"display_name":"Agent kwargs","dynamic":false,"info":"Additional kwargs for the agent.","list":true,"name":"agent_kwargs","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_input":true,"type":"dict","value":{}},"allow_code_execution":{"_input_type":"BoolInput","advanced":true,"display_name":"Allow Code Execution","dynamic":false,"info":"Whether the agent is allowed to execute code.","list":false,"name":"allow_code_execution","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_metadata":true,"type":"bool","value":false},"allow_delegation":{"_input_type":"BoolInput","advanced":true,"display_name":"Allow Delegation","dynamic":false,"info":"Whether the agent is allowed to delegate tasks to other agents.","list":false,"name":"allow_delegation","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_metadata":true,"type":"bool","value":false},"async_execution":{"_input_type":"BoolInput","advanced":true,"display_name":"Async Execution","dynamic":false,"info":"Boolean flag indicating asynchronous task execution.","list":false,"name":"async_execution","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_metadata":true,"type":"bool","value":false},"backstory":{"_input_type":"MultilineInput","advanced":false,"display_name":"Backstory","dynamic":false,"info":"The backstory of the agent.","input_types":["Message"],"list":false,"load_from_db":false,"multiline":true,"name":"backstory","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_input":true,"trace_as_metadata":true,"type":"str","value":"Developed as a response to the growing need for real-time asset valuation in volatile markets, the agent was trained on vast datasets of historical market trends, corporate financials, and macroeconomic indicators. Initially designed to assist institutional investors, the agent quickly proved invaluable in predicting market shifts and asset pricing with high accuracy. Its algorithmic foundation is built on deep learning models that factor in both quantitative data and qualitative news analysis, making it highly versatile in various financial environments."},"code":{"advanced":true,"dynamic":true,"fileTypes":[],"file_path":"","info":"","list":false,"load_from_db":false,"multiline":true,"name":"code","password":false,"placeholder":"","required":true,"show":true,"title_case":false,"type":"code","value":"from crewai import Agent, Task\n\nfrom langflow.base.agents.crewai.tasks import SequentialTask\nfrom langflow.custom import Component\nfrom langflow.io import BoolInput, DictInput, HandleInput, MultilineInput, Output\n\n\nclass SequentialTaskAgentComponent(Component):\n    display_name = \"Sequential Task Agent\"\n    description = \"Creates a CrewAI Task and its associated Agent.\"\n    documentation = \"https://docs.crewai.com/how-to/LLM-Connections/\"\n    icon = \"CrewAI\"\n\n    inputs = [\n        # Agent inputs\n        MultilineInput(name=\"role\", display_name=\"Role\", info=\"The role of the agent.\"),\n        MultilineInput(name=\"goal\", display_name=\"Goal\", info=\"The objective of the agent.\"),\n        MultilineInput(\n            name=\"backstory\",\n            display_name=\"Backstory\",\n            info=\"The backstory of the agent.\",\n        ),\n        HandleInput(\n            name=\"tools\",\n            display_name=\"Tools\",\n            input_types=[\"Tool\"],\n            is_list=True,\n            info=\"Tools at agent's disposal\",\n            value=[],\n        ),\n        HandleInput(\n            name=\"llm\",\n            display_name=\"Language Model\",\n            info=\"Language model that will run the agent.\",\n            input_types=[\"LanguageModel\"],\n        ),\n        BoolInput(\n            name=\"memory\",\n            display_name=\"Memory\",\n            info=\"Whether the agent should have memory or not\",\n            advanced=True,\n            value=True,\n        ),\n        BoolInput(\n            name=\"verbose\",\n            display_name=\"Verbose\",\n            advanced=True,\n            value=True,\n        ),\n        BoolInput(\n            name=\"allow_delegation\",\n            display_name=\"Allow Delegation\",\n            info=\"Whether the agent is allowed to delegate tasks to other agents.\",\n            value=False,\n            advanced=True,\n        ),\n        BoolInput(\n            name=\"allow_code_execution\",\n            display_name=\"Allow Code Execution\",\n            info=\"Whether the agent is allowed to execute code.\",\n            value=False,\n            advanced=True,\n        ),\n        DictInput(\n            name=\"agent_kwargs\",\n            display_name=\"Agent kwargs\",\n            info=\"Additional kwargs for the agent.\",\n            is_list=True,\n            advanced=True,\n        ),\n        # Task inputs\n        MultilineInput(\n            name=\"task_description\",\n            display_name=\"Task Description\",\n            info=\"Descriptive text detailing task's purpose and execution.\",\n        ),\n        MultilineInput(\n            name=\"expected_output\",\n            display_name=\"Expected Task Output\",\n            info=\"Clear definition of expected task outcome.\",\n        ),\n        BoolInput(\n            name=\"async_execution\",\n            display_name=\"Async Execution\",\n            value=False,\n            advanced=True,\n            info=\"Boolean flag indicating asynchronous task execution.\",\n        ),\n        # Chaining input\n        HandleInput(\n            name=\"previous_task\",\n            display_name=\"Previous Task\",\n            input_types=[\"SequentialTask\"],\n            info=\"The previous task in the sequence (for chaining).\",\n            required=False,\n        ),\n    ]\n\n    outputs = [\n        Output(\n            display_name=\"Sequential Task\",\n            name=\"task_output\",\n            method=\"build_agent_and_task\",\n        ),\n    ]\n\n    def build_agent_and_task(self) -> list[SequentialTask]:\n        # Build the agent\n        agent_kwargs = self.agent_kwargs or {}\n        agent = Agent(\n            role=self.role,\n            goal=self.goal,\n            backstory=self.backstory,\n            llm=self.llm,\n            verbose=self.verbose,\n            memory=self.memory,\n            tools=self.tools if self.tools else [],\n            allow_delegation=self.allow_delegation,\n            allow_code_execution=self.allow_code_execution,\n            **agent_kwargs,\n        )\n\n        # Build the task\n        task = Task(\n            description=self.task_description,\n            expected_output=self.expected_output,\n            agent=agent,\n            async_execution=self.async_execution,\n        )\n\n        # If there's a previous task, create a list of tasks\n        if self.previous_task:\n            if isinstance(self.previous_task, list):\n                tasks = self.previous_task + [task]\n            else:\n                tasks = [self.previous_task, task]\n        else:\n            tasks = [task]\n\n        self.status = f\"Agent: {repr(agent)}\\nTask: {repr(task)}\"\n        return tasks\n"},"expected_output":{"_input_type":"MultilineInput","advanced":false,"display_name":"Expected Task Output","dynamic":false,"info":"Clear definition of expected task outcome.","input_types":["Message"],"list":false,"load_from_db":false,"multiline":true,"name":"expected_output","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_input":true,"trace_as_metadata":true,"type":"str","value":"Escreva as informações em tópicos curtos e informativos, em portugues."},"goal":{"_input_type":"MultilineInput","advanced":false,"display_name":"Goal","dynamic":false,"info":"The objective of the agent.","input_types":["Message"],"list":false,"load_from_db":false,"multiline":true,"name":"goal","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_input":true,"trace_as_metadata":true,"type":"str","value":"The agent's primary goal is to continuously monitor financial markets, assess the value of various assets, and predict future price movements. It aims to enhance decision-making for investors by offering accurate pricing models, risk assessments, and timely advice based on both historical and real-time data. The agent also adapts its strategies based on market shifts to optimize performance in varying conditions."},"llm":{"_input_type":"HandleInput","advanced":false,"display_name":"Language Model","dynamic":false,"info":"Language model that will run the agent.","input_types":["LanguageModel"],"list":false,"name":"llm","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_metadata":true,"type":"other","value":""},"memory":{"_input_type":"BoolInput","advanced":true,"display_name":"Memory","dynamic":false,"info":"Whether the agent should have memory or not","list":false,"name":"memory","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_metadata":true,"type":"bool","value":true},"previous_task":{"_input_type":"HandleInput","advanced":false,"display_name":"Previous Task","dynamic":false,"info":"The previous task in the sequence (for chaining).","input_types":["SequentialTask"],"list":false,"name":"previous_task","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_metadata":true,"type":"other","value":""},"role":{"_input_type":"MultilineInput","advanced":false,"display_name":"Role","dynamic":false,"info":"The role of the agent.","input_types":["Message"],"list":false,"load_from_db":false,"multiline":true,"name":"role","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_input":true,"trace_as_metadata":true,"type":"str","value":"The agent is an expert in financial asset pricing, capable of analyzing market data, identifying trends, and providing real-time insights into asset valuations. It uses advanced models to calculate price predictions, evaluate risks, and offer investment recommendations for a range of financial instruments such as stocks, bonds, and commodities."},"task_description":{"_input_type":"MultilineInput","advanced":false,"display_name":"Task Description","dynamic":false,"info":"Descriptive text detailing task's purpose and execution.","input_types":["Message"],"list":false,"load_from_db":false,"multiline":true,"name":"task_description","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_input":true,"trace_as_metadata":true,"type":"str","value":""},"tools":{"_input_type":"HandleInput","advanced":false,"display_name":"Tools","dynamic":false,"info":"Tools at agent's disposal","input_types":["Tool"],"list":true,"name":"tools","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_metadata":true,"type":"other","value":[]},"verbose":{"_input_type":"BoolInput","advanced":true,"display_name":"Verbose","dynamic":false,"info":"","list":false,"name":"verbose","placeholder":"","required":false,"show":true,"title_case":false,"trace_as_metadata":true,"type":"bool","value":true}}},"type":"SequentialTaskAgentComponent"},"dragging":false,"height":785,"id":"SequentialTaskAgentComponent-saOps","position":{"x":800.6536575540351,"y":-345.48877618011403},"positionAbsolute":{"x":800.6536575540351,"y":-345.48877618011403},"selected":false,"type":"genericNode","width":384},{"id":"DuckDuckGoSearchAPI-JZWLu","type":"genericNode","position":{"x":-1433.0287321846008,"y":-425.2859936059077},"data":{"type":"DuckDuckGoSearchAPI","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from typing import Union\n\nfrom langchain_core.tools import Tool\n\nfrom langflow.base.langchain_utilities.model import LCToolComponent\nfrom langflow.inputs import SecretStrInput, MultilineInput, IntInput\nfrom langflow.schema import Data\n\n\nclass GoogleSearchAPIComponent(LCToolComponent):\n    display_name = \"DuckDuckGo Search API\"\n    description = \"Call DuckDuckGo Search API for web browsing.\"\n    name = \"DuckDuckGoSearchAPI\"\n\n    inputs = [\n        MultilineInput(\n            name=\"input_value\",\n            display_name=\"Query\",\n        )\n    ]\n\n    def run_model(self) -> Union[Data, list[Data]]:\n        wrapper = self._build_wrapper()\n        results = wrapper.invoke(query=self.input_value)\n\n        # Step 1: Split the string into individual entries based on \"], [\" which separates them\n        entries = results.strip(\"[]\").split(\"], [\")\n        \n        # Step 2: Convert each entry into a dictionary\n        results_converted = []\n        for entry in entries:\n            # Use regex to find all key-value pairs\n            pairs = re.findall(r'(\\w+):\\s(.*?)(?=, \\w+:|$)', entry)\n            \n            # Convert the list of pairs into a dictionary\n            entry_dict = {key: value.strip() for key, value in pairs}\n            \n            # Append the dictionary to the converted list\n            results_converted.append(entry_dict)\n\n        data = [Data(data=result, text=\"{}, {}, {}\".format(result[\"title\"], result[\"snippet\"], result[\"link\"])) for result in results_converted]\n        print(\"data\", data)\n        self.status = data\n        return data\n\n    def build_tool(self) -> Tool:\n        wrapper = self._build_wrapper()\n        return Tool(\n            name=\"duckduckgo_search\",\n            description=\"Browse with DuckDuckGo.\",\n            func=wrapper.run,\n        )\n\n    def _build_wrapper(self):\n        try:\n            from langchain_community.tools import DuckDuckGoSearchResults  # type: ignore\n        except ImportError:\n            raise ImportError(\"Please install duckduckgo-search to use DuckDuckGoSearchResults.\")\n        return DuckDuckGoSearchResults()\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"input_value":{"trace_as_input":true,"multiline":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"input_value","display_name":"Query","advanced":false,"input_types":["Message"],"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"MultilineInput"}},"description":"Call DuckDuckGo Search API for web browsing.","base_classes":["Data","Tool"],"display_name":"DuckDuckGo Free Search Tool","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Data"],"selected":"Data","name":"api_run_model","display_name":"Data","method":"run_model","value":"__UNDEFINED__","cache":true},{"types":["Tool"],"selected":"Tool","name":"api_build_tool","display_name":"Tool","method":"build_tool","value":"__UNDEFINED__","cache":true}],"field_order":["input_value"],"beta":false,"edited":true,"official":false,"lf_version":"1.0.17"},"id":"DuckDuckGoSearchAPI-JZWLu"},"selected":false,"width":384,"height":344,"positionAbsolute":{"x":-1433.0287321846008,"y":-425.2859936059077},"dragging":false},{"id":"ChatInput-b5XMZ","type":"genericNode","position":{"x":-2542.951608275084,"y":438.0046500932713},"data":{"type":"ChatInput","node":{"template":{"_type":"Component","files":{"trace_as_metadata":true,"file_path":"","fileTypes":["txt","md","mdx","csv","json","yaml","yml","xml","html","htm","pdf","docx","py","sh","sql","js","ts","tsx","jpg","jpeg","png","bmp","image"],"list":true,"required":false,"placeholder":"","show":true,"name":"files","value":"","display_name":"Files","advanced":true,"dynamic":false,"info":"Files to be sent with the message.","title_case":false,"type":"file","_input_type":"FileInput"},"code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.base.data.utils import IMG_FILE_TYPES, TEXT_FILE_TYPES\nfrom langflow.base.io.chat import ChatComponent\nfrom langflow.inputs import BoolInput\nfrom langflow.io import DropdownInput, FileInput, MessageTextInput, MultilineInput, Output\nfrom langflow.memory import store_message\nfrom langflow.schema.message import Message\nfrom langflow.utils.constants import MESSAGE_SENDER_AI, MESSAGE_SENDER_USER, MESSAGE_SENDER_NAME_USER\n\n\nclass ChatInput(ChatComponent):\n    display_name = \"Chat Input\"\n    description = \"Get chat inputs from the Playground.\"\n    icon = \"ChatInput\"\n    name = \"ChatInput\"\n\n    inputs = [\n        MultilineInput(\n            name=\"input_value\",\n            display_name=\"Text\",\n            value=\"\",\n            info=\"Message to be passed as input.\",\n        ),\n        BoolInput(\n            name=\"should_store_message\",\n            display_name=\"Store Messages\",\n            info=\"Store the message in the history.\",\n            value=True,\n            advanced=True,\n        ),\n        DropdownInput(\n            name=\"sender\",\n            display_name=\"Sender Type\",\n            options=[MESSAGE_SENDER_AI, MESSAGE_SENDER_USER],\n            value=MESSAGE_SENDER_USER,\n            info=\"Type of sender.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"sender_name\",\n            display_name=\"Sender Name\",\n            info=\"Name of the sender.\",\n            value=MESSAGE_SENDER_NAME_USER,\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"session_id\",\n            display_name=\"Session ID\",\n            info=\"The session ID of the chat. If empty, the current session ID parameter will be used.\",\n            advanced=True,\n        ),\n        FileInput(\n            name=\"files\",\n            display_name=\"Files\",\n            file_types=TEXT_FILE_TYPES + IMG_FILE_TYPES,\n            info=\"Files to be sent with the message.\",\n            advanced=True,\n            is_list=True,\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Message\", name=\"message\", method=\"message_response\"),\n    ]\n\n    def message_response(self) -> Message:\n        message = Message(\n            text=self.input_value,\n            sender=self.sender,\n            sender_name=self.sender_name,\n            session_id=self.session_id,\n            files=self.files,\n        )\n\n        if (\n            self.session_id\n            and isinstance(message, Message)\n            and isinstance(message.text, str)\n            and self.should_store_message\n        ):\n            store_message(\n                message,\n                flow_id=self.graph.flow_id,\n            )\n            self.message.value = message\n\n        self.status = message\n        return message\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"input_value":{"trace_as_input":true,"multiline":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"input_value","value":"oi","display_name":"Text","advanced":false,"input_types":["Message"],"dynamic":false,"info":"Message to be passed as input.","title_case":false,"type":"str","_input_type":"MultilineInput"},"sender":{"trace_as_metadata":true,"options":["Machine","User"],"combobox":false,"required":false,"placeholder":"","show":true,"name":"sender","value":"User","display_name":"Sender Type","advanced":true,"dynamic":false,"info":"Type of sender.","title_case":false,"type":"str","_input_type":"DropdownInput"},"sender_name":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"sender_name","value":"User","display_name":"Sender Name","advanced":true,"input_types":["Message"],"dynamic":false,"info":"Name of the sender.","title_case":false,"type":"str","_input_type":"MessageTextInput"},"session_id":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"session_id","value":"","display_name":"Session ID","advanced":true,"input_types":["Message"],"dynamic":false,"info":"The session ID of the chat. If empty, the current session ID parameter will be used.","title_case":false,"type":"str","_input_type":"MessageTextInput"},"should_store_message":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"should_store_message","value":true,"display_name":"Store Messages","advanced":true,"dynamic":false,"info":"Store the message in the history.","title_case":false,"type":"bool","_input_type":"BoolInput"}},"description":"Get chat inputs from the Playground.","icon":"ChatInput","base_classes":["Message"],"display_name":"Chat Input","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"message","display_name":"Message","method":"message_response","value":"__UNDEFINED__","cache":true}],"field_order":["input_value","should_store_message","sender","sender_name","session_id","files"],"beta":false,"edited":false,"lf_version":"1.0.17"},"id":"ChatInput-b5XMZ"},"selected":false,"width":384,"height":299,"dragging":false},{"id":"AIMLModel-FpMCK","type":"genericNode","position":{"x":-2046.9436628926874,"y":-559.8286934885541},"data":{"type":"AIMLModel","node":{"template":{"_type":"Component","aiml_api_base":{"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"aiml_api_base","value":"","display_name":"AIML API Base","advanced":true,"dynamic":false,"info":"The base URL of the OpenAI API. Defaults to https://api.aimlapi.com . You can change this to use other APIs like JinaChat, LocalAI e Prem.","title_case":false,"type":"str","_input_type":"StrInput"},"api_key":{"load_from_db":false,"required":false,"placeholder":"","show":true,"name":"api_key","value":"c6d20860e1f24a6da42f1d496cf0a66b","display_name":"AIML API Key","advanced":false,"input_types":["Message"],"dynamic":false,"info":"The AIML API Key to use for the OpenAI model.","title_case":false,"password":true,"type":"str","_input_type":"SecretStrInput"},"code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.field_typing.range_spec import RangeSpec\nfrom langchain_openai import ChatOpenAI\nfrom pydantic.v1 import SecretStr\n\nfrom langflow.base.models.aiml_constants import AIML_CHAT_MODELS\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.field_typing import LanguageModel\nfrom langflow.inputs import (\n    DictInput,\n    DropdownInput,\n    FloatInput,\n    IntInput,\n    SecretStrInput,\n    StrInput,\n)\n\n\nclass AIMLModelComponent(LCModelComponent):\n    display_name = \"AIML\"\n    description = \"Generates text using AIML LLMs.\"\n    icon = \"AIML\"\n    name = \"AIMLModel\"\n    documentation = \"https://docs.aimlapi.com/api-reference\"\n\n    inputs = LCModelComponent._base_inputs + [\n        IntInput(\n            name=\"max_tokens\",\n            display_name=\"Max Tokens\",\n            advanced=True,\n            info=\"The maximum number of tokens to generate. Set to 0 for unlimited tokens.\",\n            range_spec=RangeSpec(min=0, max=128000),\n        ),\n        DictInput(name=\"model_kwargs\", display_name=\"Model Kwargs\", advanced=True),\n        DropdownInput(\n            name=\"model_name\",\n            display_name=\"Model Name\",\n            advanced=False,\n            options=AIML_CHAT_MODELS,\n            value=AIML_CHAT_MODELS[0],\n        ),\n        StrInput(\n            name=\"aiml_api_base\",\n            display_name=\"AIML API Base\",\n            advanced=True,\n            info=\"The base URL of the OpenAI API. Defaults to https://api.aimlapi.com . You can change this to use other APIs like JinaChat, LocalAI e Prem.\",\n        ),\n        SecretStrInput(\n            name=\"api_key\",\n            display_name=\"AIML API Key\",\n            info=\"The AIML API Key to use for the OpenAI model.\",\n            advanced=False,\n            value=\"AIML_API_KEY\",\n        ),\n        FloatInput(name=\"temperature\", display_name=\"Temperature\", value=0.1),\n        IntInput(\n            name=\"seed\",\n            display_name=\"Seed\",\n            info=\"The seed controls the reproducibility of the job.\",\n            advanced=True,\n            value=1,\n        ),\n    ]\n\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\n        aiml_api_key = self.api_key\n        temperature = self.temperature\n        model_name: str = self.model_name\n        max_tokens = self.max_tokens\n        model_kwargs = self.model_kwargs or {}\n        aiml_api_base = self.aiml_api_base or \"https://api.aimlapi.com\"\n        seed = self.seed\n\n        if isinstance(aiml_api_key, SecretStr):\n            openai_api_key = aiml_api_key.get_secret_value()\n        else:\n            openai_api_key = aiml_api_key\n\n        model = ChatOpenAI(\n            model=model_name,\n            temperature=temperature,\n            api_key=openai_api_key,\n            base_url=aiml_api_base,\n            max_tokens=max_tokens or None,\n            seed=seed,\n            **model_kwargs,\n        )\n\n        return model  # type: ignore\n\n    def _get_exception_message(self, e: Exception):\n        \"\"\"\n        Get a message from an OpenAI exception.\n\n        Args:\n            exception (Exception): The exception to get the message from.\n\n        Returns:\n            str: The message from the exception.\n        \"\"\"\n        try:\n            from openai.error import BadRequestError\n        except ImportError:\n            return None\n        if isinstance(e, BadRequestError):\n            message = e.json_body.get(\"error\", {}).get(\"message\", \"\")  # type: ignore\n            if message:\n                return message\n        return None\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"input_value":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"input_value","value":{"text_key":"text","data":{"text":"qual o meu nome?\nAI: Desculpe, mas não tenho acesso a informações pessoais, incluindo seu nome. Como posso ajudá-lo de outra forma?\nUser: me chame de lindo\nAI: Claro, lindo! Como posso ajudá-lo hoje?\nUser: qual o meu nome?\n","files":[],"timestamp":"2024-09-07 18:31:19","flow_id":"1c091528-eacd-4c5c-a018-8b5362438ae9"},"default_value":"","text":"qual o meu nome?\nAI: Desculpe, mas não tenho acesso a informações pessoais, incluindo seu nome. Como posso ajudá-lo de outra forma?\nUser: me chame de lindo\nAI: Claro, lindo! Como posso ajudá-lo hoje?\nUser: qual o meu nome?\n","files":[],"session_id":"","timestamp":"2024-09-07 18:31:19","flow_id":"1c091528-eacd-4c5c-a018-8b5362438ae9"},"display_name":"Input","advanced":false,"input_types":["Message"],"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"MessageInput"},"max_tokens":{"trace_as_metadata":true,"range_spec":{"step_type":"float","min":0,"max":128000,"step":0.1},"list":false,"required":false,"placeholder":"","show":true,"name":"max_tokens","value":"","display_name":"Max Tokens","advanced":true,"dynamic":false,"info":"The maximum number of tokens to generate. Set to 0 for unlimited tokens.","title_case":false,"type":"int","_input_type":"IntInput"},"model_kwargs":{"trace_as_input":true,"list":false,"required":false,"placeholder":"","show":true,"name":"model_kwargs","value":{},"display_name":"Model Kwargs","advanced":true,"dynamic":false,"info":"","title_case":false,"type":"dict","_input_type":"DictInput"},"model_name":{"trace_as_metadata":true,"options":["zero-one-ai/Yi-34B-Chat","allenai/OLMo-7B-Instruct","allenai/OLMo-7B-Twin-2T","allenai/OLMo-7B","Austism/chronos-hermes-13b","cognitivecomputations/dolphin-2.5-mixtral-8x7b","deepseek-ai/deepseek-coder-33b-instruct","deepseek-ai/deepseek-llm-67b-chat","garage-bAInd/Platypus2-70B-instruct","google/gemma-2b-it","google/gemma-7b-it","Gryphe/MythoMax-L2-13b","lmsys/vicuna-13b-v1.5","lmsys/vicuna-7b-v1.5","codellama/CodeLlama-13b-Instruct-hf","codellama/CodeLlama-34b-Instruct-hf","codellama/CodeLlama-70b-Instruct-hf","codellama/CodeLlama-7b-Instruct-hf","meta-llama/Meta-Llama-3.1-405B-Instruct-Turbo","meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo","meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo","meta-llama/Llama-2-70b-chat-hf","meta-llama/Llama-2-13b-chat-hf","meta-llama/Llama-2-7b-chat-hf","mistralai/Mistral-7B-Instruct-v0.1","mistralai/Mistral-7B-Instruct-v0.2","mistralai/Mixtral-8x7B-Instruct-v0.1","NousResearch/Nous-Capybara-7B-V1p9","NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO","NousResearch/Nous-Hermes-2-Mixtral-8x7B-SFT","NousResearch/Nous-Hermes-llama-2-7b","NousResearch/Nous-Hermes-Llama2-13b","NousResearch/Nous-Hermes-2-Yi-34B","openchat/openchat-3.5-1210","Open-Orca/Mistral-7B-OpenOrca","togethercomputer/Qwen-7B-Chat","Qwen/Qwen1.5-0.5B-Chat","Qwen/Qwen1.5-1.8B-Chat","Qwen/Qwen1.5-4B-Chat","Qwen/Qwen1.5-7B-Chat","Qwen/Qwen1.5-14B-Chat","Qwen/Qwen1.5-72B-Chat","snorkelai/Snorkel-Mistral-PairRM-DPO","togethercomputer/alpaca-7b","teknium/OpenHermes-2-Mistral-7B","teknium/OpenHermes-2p5-Mistral-7B","togethercomputer/falcon-40b-instruct","togethercomputer/falcon-7b-instruct","togethercomputer/Llama-2-7B-32K-Instruct","togethercomputer/RedPajama-INCITE-Chat-3B-v1","togethercomputer/RedPajama-INCITE-7B-Chat","togethercomputer/StripedHyena-Nous-7B","Undi95/ReMM-SLERP-L2-13B","Undi95/Toppy-M-7B","WizardLM/WizardLM-13B-V1.2","upstage/SOLAR-10.7B-Instruct-v1.0","gpt-4","gpt-4-turbo","gpt-4-0613","gpt-4-32k","gpt-4-32k-0613","gpt-3.5-turbo-0125","gpt-3.5-turbo","gpt-3.5-turbo-1106","gpt-3.5-turbo-instruct","gpt-3.5-turbo-16k","gpt-3.5-turbo-0613","gpt-3.5-turbo-16k-0613","gpt-4o","claude-3-opus-20240229","claude-3-sonnet-20240229","claude-3-haiku-20240307"],"combobox":false,"required":false,"placeholder":"","show":true,"name":"model_name","value":"gpt-4-turbo","display_name":"Model Name","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"DropdownInput"},"seed":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"seed","value":1,"display_name":"Seed","advanced":true,"dynamic":false,"info":"The seed controls the reproducibility of the job.","title_case":false,"type":"int","_input_type":"IntInput"},"stream":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"stream","value":false,"display_name":"Stream","advanced":true,"dynamic":false,"info":"Stream the response from the model. Streaming works only in Chat.","title_case":false,"type":"bool","_input_type":"BoolInput"},"system_message":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"system_message","value":"","display_name":"System Message","advanced":true,"input_types":["Message"],"dynamic":false,"info":"System message to pass to the model.","title_case":false,"type":"str","_input_type":"MessageTextInput"},"temperature":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"temperature","value":0.1,"display_name":"Temperature","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"float","_input_type":"FloatInput"}},"description":"Generates text using AIML LLMs.","icon":"AIML","base_classes":["LanguageModel","Message"],"display_name":"AIML","documentation":"https://docs.aimlapi.com/api-reference","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"text_output","display_name":"Text","method":"text_response","value":"__UNDEFINED__","cache":true},{"types":["LanguageModel"],"selected":"LanguageModel","name":"model_output","display_name":"Language Model","method":"build_model","value":"__UNDEFINED__","cache":true}],"field_order":["input_value","system_message","stream","max_tokens","model_kwargs","model_name","aiml_api_base","api_key","temperature","seed"],"beta":false,"edited":false,"lf_version":"1.0.17"},"id":"AIMLModel-FpMCK"},"selected":false,"width":384,"height":603,"positionAbsolute":{"x":-2046.9436628926874,"y":-559.8286934885541},"dragging":false}],"edges":[{"className":"","data":{"sourceHandle":{"dataType":"SequentialCrewComponent","id":"SequentialCrewComponent-AoXgm","name":"output","output_types":["Message"]},"targetHandle":{"fieldName":"input_value","id":"ChatOutput-kcdMZ","inputTypes":["Message"],"type":"str"}},"id":"reactflow__edge-SequentialCrewComponent-AoXgm{œdataTypeœ:œSequentialCrewComponentœ,œidœ:œSequentialCrewComponent-AoXgmœ,œnameœ:œoutputœ,œoutput_typesœ:[œMessageœ]}-ChatOutput-kcdMZ{œfieldNameœ:œinput_valueœ,œidœ:œChatOutput-kcdMZœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","source":"SequentialCrewComponent-AoXgm","sourceHandle":"{œdataTypeœ:œSequentialCrewComponentœ,œidœ:œSequentialCrewComponent-AoXgmœ,œnameœ:œoutputœ,œoutput_typesœ:[œMessageœ]}","target":"ChatOutput-kcdMZ","targetHandle":"{œfieldNameœ:œinput_valueœ,œidœ:œChatOutput-kcdMZœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}"},{"className":"","data":{"sourceHandle":{"dataType":"TextInput","id":"TextInput-q5KbN","name":"text","output_types":["Message"]},"targetHandle":{"fieldName":"topic","id":"Prompt-m0j1W","inputTypes":["Message","Text"],"type":"str"}},"id":"reactflow__edge-TextInput-q5KbN{œdataTypeœ:œTextInputœ,œidœ:œTextInput-q5KbNœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}-Prompt-m0j1W{œfieldNameœ:œtopicœ,œidœ:œPrompt-m0j1Wœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","source":"TextInput-q5KbN","sourceHandle":"{œdataTypeœ:œTextInputœ,œidœ:œTextInput-q5KbNœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}","target":"Prompt-m0j1W","targetHandle":"{œfieldNameœ:œtopicœ,œidœ:œPrompt-m0j1Wœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}"},{"className":"","data":{"sourceHandle":{"dataType":"TextInput","id":"TextInput-q5KbN","name":"text","output_types":["Message"]},"targetHandle":{"fieldName":"topic","id":"Prompt-vu8ox","inputTypes":["Message","Text"],"type":"str"}},"id":"reactflow__edge-TextInput-q5KbN{œdataTypeœ:œTextInputœ,œidœ:œTextInput-q5KbNœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}-Prompt-vu8ox{œfieldNameœ:œtopicœ,œidœ:œPrompt-vu8oxœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","source":"TextInput-q5KbN","sourceHandle":"{œdataTypeœ:œTextInputœ,œidœ:œTextInput-q5KbNœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}","target":"Prompt-vu8ox","targetHandle":"{œfieldNameœ:œtopicœ,œidœ:œPrompt-vu8oxœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}"},{"className":"","data":{"sourceHandle":{"dataType":"TextInput","id":"TextInput-q5KbN","name":"text","output_types":["Message"]},"targetHandle":{"fieldName":"topic","id":"Prompt-ajPtk","inputTypes":["Message","Text"],"type":"str"}},"id":"reactflow__edge-TextInput-q5KbN{œdataTypeœ:œTextInputœ,œidœ:œTextInput-q5KbNœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}-Prompt-ajPtk{œfieldNameœ:œtopicœ,œidœ:œPrompt-ajPtkœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","source":"TextInput-q5KbN","sourceHandle":"{œdataTypeœ:œTextInputœ,œidœ:œTextInput-q5KbNœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}","target":"Prompt-ajPtk","targetHandle":"{œfieldNameœ:œtopicœ,œidœ:œPrompt-ajPtkœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}"},{"className":"","data":{"sourceHandle":{"dataType":"Prompt","id":"Prompt-m0j1W","name":"prompt","output_types":["Message"]},"targetHandle":{"fieldName":"task_description","id":"SequentialTaskAgentComponent-pkN2P","inputTypes":["Message"],"type":"str"}},"id":"reactflow__edge-Prompt-m0j1W{œdataTypeœ:œPromptœ,œidœ:œPrompt-m0j1Wœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}-SequentialTaskAgentComponent-pkN2P{œfieldNameœ:œtask_descriptionœ,œidœ:œSequentialTaskAgentComponent-pkN2Pœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","source":"Prompt-m0j1W","sourceHandle":"{œdataTypeœ:œPromptœ,œidœ:œPrompt-m0j1Wœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}","target":"SequentialTaskAgentComponent-pkN2P","targetHandle":"{œfieldNameœ:œtask_descriptionœ,œidœ:œSequentialTaskAgentComponent-pkN2Pœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}"},{"data":{"sourceHandle":{"dataType":"Prompt","id":"Prompt-vu8ox","name":"prompt","output_types":["Message"]},"targetHandle":{"fieldName":"task_description","id":"SequentialTaskAgentComponent-tuwOX","inputTypes":["Message"],"type":"str"}},"id":"reactflow__edge-Prompt-vu8ox{œdataTypeœ:œPromptœ,œidœ:œPrompt-vu8oxœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}-SequentialTaskAgentComponent-tuwOX{œfieldNameœ:œtask_descriptionœ,œidœ:œSequentialTaskAgentComponent-tuwOXœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","source":"Prompt-vu8ox","sourceHandle":"{œdataTypeœ:œPromptœ,œidœ:œPrompt-vu8oxœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}","target":"SequentialTaskAgentComponent-tuwOX","targetHandle":"{œfieldNameœ:œtask_descriptionœ,œidœ:œSequentialTaskAgentComponent-tuwOXœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","className":""},{"data":{"sourceHandle":{"dataType":"SequentialTaskAgentComponent","id":"SequentialTaskAgentComponent-pkN2P","name":"task_output","output_types":["SequentialTask"]},"targetHandle":{"fieldName":"previous_task","id":"SequentialTaskAgentComponent-tuwOX","inputTypes":["SequentialTask"],"type":"other"}},"id":"reactflow__edge-SequentialTaskAgentComponent-pkN2P{œdataTypeœ:œSequentialTaskAgentComponentœ,œidœ:œSequentialTaskAgentComponent-pkN2Pœ,œnameœ:œtask_outputœ,œoutput_typesœ:[œSequentialTaskœ]}-SequentialTaskAgentComponent-tuwOX{œfieldNameœ:œprevious_taskœ,œidœ:œSequentialTaskAgentComponent-tuwOXœ,œinputTypesœ:[œSequentialTaskœ],œtypeœ:œotherœ}","source":"SequentialTaskAgentComponent-pkN2P","sourceHandle":"{œdataTypeœ:œSequentialTaskAgentComponentœ,œidœ:œSequentialTaskAgentComponent-pkN2Pœ,œnameœ:œtask_outputœ,œoutput_typesœ:[œSequentialTaskœ]}","target":"SequentialTaskAgentComponent-tuwOX","targetHandle":"{œfieldNameœ:œprevious_taskœ,œidœ:œSequentialTaskAgentComponent-tuwOXœ,œinputTypesœ:[œSequentialTaskœ],œtypeœ:œotherœ}","className":""},{"data":{"sourceHandle":{"dataType":"SequentialTaskAgentComponent","id":"SequentialTaskAgentComponent-tuwOX","name":"task_output","output_types":["SequentialTask"]},"targetHandle":{"fieldName":"previous_task","id":"SequentialTaskAgentComponent-saOps","inputTypes":["SequentialTask"],"type":"other"}},"id":"reactflow__edge-SequentialTaskAgentComponent-tuwOX{œdataTypeœ:œSequentialTaskAgentComponentœ,œidœ:œSequentialTaskAgentComponent-tuwOXœ,œnameœ:œtask_outputœ,œoutput_typesœ:[œSequentialTaskœ]}-SequentialTaskAgentComponent-saOps{œfieldNameœ:œprevious_taskœ,œidœ:œSequentialTaskAgentComponent-saOpsœ,œinputTypesœ:[œSequentialTaskœ],œtypeœ:œotherœ}","source":"SequentialTaskAgentComponent-tuwOX","sourceHandle":"{œdataTypeœ:œSequentialTaskAgentComponentœ,œidœ:œSequentialTaskAgentComponent-tuwOXœ,œnameœ:œtask_outputœ,œoutput_typesœ:[œSequentialTaskœ]}","target":"SequentialTaskAgentComponent-saOps","targetHandle":"{œfieldNameœ:œprevious_taskœ,œidœ:œSequentialTaskAgentComponent-saOpsœ,œinputTypesœ:[œSequentialTaskœ],œtypeœ:œotherœ}","className":""},{"data":{"sourceHandle":{"dataType":"Prompt","id":"Prompt-ajPtk","name":"prompt","output_types":["Message"]},"targetHandle":{"fieldName":"task_description","id":"SequentialTaskAgentComponent-saOps","inputTypes":["Message"],"type":"str"}},"id":"reactflow__edge-Prompt-ajPtk{œdataTypeœ:œPromptœ,œidœ:œPrompt-ajPtkœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}-SequentialTaskAgentComponent-saOps{œfieldNameœ:œtask_descriptionœ,œidœ:œSequentialTaskAgentComponent-saOpsœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","source":"Prompt-ajPtk","sourceHandle":"{œdataTypeœ:œPromptœ,œidœ:œPrompt-ajPtkœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}","target":"SequentialTaskAgentComponent-saOps","targetHandle":"{œfieldNameœ:œtask_descriptionœ,œidœ:œSequentialTaskAgentComponent-saOpsœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","className":""},{"data":{"sourceHandle":{"dataType":"SequentialTaskAgentComponent","id":"SequentialTaskAgentComponent-saOps","name":"task_output","output_types":["SequentialTask"]},"targetHandle":{"fieldName":"tasks","id":"SequentialCrewComponent-AoXgm","inputTypes":["SequentialTask"],"type":"other"}},"id":"reactflow__edge-SequentialTaskAgentComponent-saOps{œdataTypeœ:œSequentialTaskAgentComponentœ,œidœ:œSequentialTaskAgentComponent-saOpsœ,œnameœ:œtask_outputœ,œoutput_typesœ:[œSequentialTaskœ]}-SequentialCrewComponent-AoXgm{œfieldNameœ:œtasksœ,œidœ:œSequentialCrewComponent-AoXgmœ,œinputTypesœ:[œSequentialTaskœ],œtypeœ:œotherœ}","source":"SequentialTaskAgentComponent-saOps","sourceHandle":"{œdataTypeœ:œSequentialTaskAgentComponentœ,œidœ:œSequentialTaskAgentComponent-saOpsœ,œnameœ:œtask_outputœ,œoutput_typesœ:[œSequentialTaskœ]}","target":"SequentialCrewComponent-AoXgm","targetHandle":"{œfieldNameœ:œtasksœ,œidœ:œSequentialCrewComponent-AoXgmœ,œinputTypesœ:[œSequentialTaskœ],œtypeœ:œotherœ}","className":""},{"source":"ChatInput-b5XMZ","sourceHandle":"{œdataTypeœ:œChatInputœ,œidœ:œChatInput-b5XMZœ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}","target":"TextInput-q5KbN","targetHandle":"{œfieldNameœ:œinput_valueœ,œidœ:œTextInput-q5KbNœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"input_value","id":"TextInput-q5KbN","inputTypes":["Message"],"type":"str"},"sourceHandle":{"dataType":"ChatInput","id":"ChatInput-b5XMZ","name":"message","output_types":["Message"]}},"id":"reactflow__edge-ChatInput-b5XMZ{œdataTypeœ:œChatInputœ,œidœ:œChatInput-b5XMZœ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}-TextInput-q5KbN{œfieldNameœ:œinput_valueœ,œidœ:œTextInput-q5KbNœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","className":""},{"source":"TextInput-q5KbN","sourceHandle":"{œdataTypeœ:œTextInputœ,œidœ:œTextInput-q5KbNœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}","target":"DuckDuckGoSearchAPI-JZWLu","targetHandle":"{œfieldNameœ:œinput_valueœ,œidœ:œDuckDuckGoSearchAPI-JZWLuœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"input_value","id":"DuckDuckGoSearchAPI-JZWLu","inputTypes":["Message"],"type":"str"},"sourceHandle":{"dataType":"TextInput","id":"TextInput-q5KbN","name":"text","output_types":["Message"]}},"className":"","id":"reactflow__edge-TextInput-q5KbN{œdataTypeœ:œTextInputœ,œidœ:œTextInput-q5KbNœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}-DuckDuckGoSearchAPI-JZWLu{œfieldNameœ:œinput_valueœ,œidœ:œDuckDuckGoSearchAPI-JZWLuœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}"},{"source":"DuckDuckGoSearchAPI-JZWLu","sourceHandle":"{œdataTypeœ:œDuckDuckGoSearchAPIœ,œidœ:œDuckDuckGoSearchAPI-JZWLuœ,œnameœ:œapi_build_toolœ,œoutput_typesœ:[œToolœ]}","target":"SequentialTaskAgentComponent-pkN2P","targetHandle":"{œfieldNameœ:œtoolsœ,œidœ:œSequentialTaskAgentComponent-pkN2Pœ,œinputTypesœ:[œToolœ],œtypeœ:œotherœ}","data":{"targetHandle":{"fieldName":"tools","id":"SequentialTaskAgentComponent-pkN2P","inputTypes":["Tool"],"type":"other"},"sourceHandle":{"dataType":"DuckDuckGoSearchAPI","id":"DuckDuckGoSearchAPI-JZWLu","name":"api_build_tool","output_types":["Tool"]}},"className":"","id":"reactflow__edge-DuckDuckGoSearchAPI-JZWLu{œdataTypeœ:œDuckDuckGoSearchAPIœ,œidœ:œDuckDuckGoSearchAPI-JZWLuœ,œnameœ:œapi_build_toolœ,œoutput_typesœ:[œToolœ]}-SequentialTaskAgentComponent-pkN2P{œfieldNameœ:œtoolsœ,œidœ:œSequentialTaskAgentComponent-pkN2Pœ,œinputTypesœ:[œToolœ],œtypeœ:œotherœ}"},{"source":"AIMLModel-FpMCK","sourceHandle":"{œdataTypeœ:œAIMLModelœ,œidœ:œAIMLModel-FpMCKœ,œnameœ:œmodel_outputœ,œoutput_typesœ:[œLanguageModelœ]}","target":"SequentialTaskAgentComponent-pkN2P","targetHandle":"{œfieldNameœ:œllmœ,œidœ:œSequentialTaskAgentComponent-pkN2Pœ,œinputTypesœ:[œLanguageModelœ],œtypeœ:œotherœ}","data":{"targetHandle":{"fieldName":"llm","id":"SequentialTaskAgentComponent-pkN2P","inputTypes":["LanguageModel"],"type":"other"},"sourceHandle":{"dataType":"AIMLModel","id":"AIMLModel-FpMCK","name":"model_output","output_types":["LanguageModel"]}},"id":"reactflow__edge-AIMLModel-FpMCK{œdataTypeœ:œAIMLModelœ,œidœ:œAIMLModel-FpMCKœ,œnameœ:œmodel_outputœ,œoutput_typesœ:[œLanguageModelœ]}-SequentialTaskAgentComponent-pkN2P{œfieldNameœ:œllmœ,œidœ:œSequentialTaskAgentComponent-pkN2Pœ,œinputTypesœ:[œLanguageModelœ],œtypeœ:œotherœ}"},{"source":"AIMLModel-FpMCK","sourceHandle":"{œdataTypeœ:œAIMLModelœ,œidœ:œAIMLModel-FpMCKœ,œnameœ:œmodel_outputœ,œoutput_typesœ:[œLanguageModelœ]}","target":"SequentialTaskAgentComponent-tuwOX","targetHandle":"{œfieldNameœ:œllmœ,œidœ:œSequentialTaskAgentComponent-tuwOXœ,œinputTypesœ:[œLanguageModelœ],œtypeœ:œotherœ}","data":{"targetHandle":{"fieldName":"llm","id":"SequentialTaskAgentComponent-tuwOX","inputTypes":["LanguageModel"],"type":"other"},"sourceHandle":{"dataType":"AIMLModel","id":"AIMLModel-FpMCK","name":"model_output","output_types":["LanguageModel"]}},"id":"reactflow__edge-AIMLModel-FpMCK{œdataTypeœ:œAIMLModelœ,œidœ:œAIMLModel-FpMCKœ,œnameœ:œmodel_outputœ,œoutput_typesœ:[œLanguageModelœ]}-SequentialTaskAgentComponent-tuwOX{œfieldNameœ:œllmœ,œidœ:œSequentialTaskAgentComponent-tuwOXœ,œinputTypesœ:[œLanguageModelœ],œtypeœ:œotherœ}"},{"source":"AIMLModel-FpMCK","sourceHandle":"{œdataTypeœ:œAIMLModelœ,œidœ:œAIMLModel-FpMCKœ,œnameœ:œmodel_outputœ,œoutput_typesœ:[œLanguageModelœ]}","target":"SequentialTaskAgentComponent-saOps","targetHandle":"{œfieldNameœ:œllmœ,œidœ:œSequentialTaskAgentComponent-saOpsœ,œinputTypesœ:[œLanguageModelœ],œtypeœ:œotherœ}","data":{"targetHandle":{"fieldName":"llm","id":"SequentialTaskAgentComponent-saOps","inputTypes":["LanguageModel"],"type":"other"},"sourceHandle":{"dataType":"AIMLModel","id":"AIMLModel-FpMCK","name":"model_output","output_types":["LanguageModel"]}},"id":"reactflow__edge-AIMLModel-FpMCK{œdataTypeœ:œAIMLModelœ,œidœ:œAIMLModel-FpMCKœ,œnameœ:œmodel_outputœ,œoutput_typesœ:[œLanguageModelœ]}-SequentialTaskAgentComponent-saOps{œfieldNameœ:œllmœ,œidœ:œSequentialTaskAgentComponent-saOpsœ,œinputTypesœ:[œLanguageModelœ],œtypeœ:œotherœ}"}],"viewport":{"x":357.91319309479854,"y":454.0157243109036,"zoom":0.3871872192129213}},"description":"This Agent runs tasks in a predefined sequence.","name":"Sequential Tasks Agent","last_tested_version":"1.0.17","endpoint_name":null,"is_component":false}